{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics - ADF Test For Stationarity\n",
      "\n",
      "ADF Stat is: -7.045989217957895.\n",
      "P Val is: 5.685226748940427e-10.\n",
      "Critical Values (Significance Levels): \n",
      "1% : -3.435\n",
      "5% : -2.864\n",
      "10% : -2.568\n",
      "Epoch 1/1000\n",
      "32/32 [==============================] - 5s 87ms/step - loss: 0.1230 - val_loss: 0.0061\n",
      "Epoch 2/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0027 - val_loss: 0.0061\n",
      "Epoch 3/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0037 - val_loss: 0.0061\n",
      "Epoch 4/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0029 - val_loss: 0.0061\n",
      "Epoch 5/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0028 - val_loss: 0.0061\n",
      "Epoch 6/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0021 - val_loss: 0.0060\n",
      "Epoch 7/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 8/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0024 - val_loss: 0.0055\n",
      "Epoch 9/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 10/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 11/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0032 - val_loss: 0.0055\n",
      "Epoch 12/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0032 - val_loss: 0.0055\n",
      "Epoch 13/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0031 - val_loss: 0.0053\n",
      "Epoch 14/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 15/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0015 - val_loss: 0.0052\n",
      "Epoch 16/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 17/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0021 - val_loss: 0.0053\n",
      "Epoch 18/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 19/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 20/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 21/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0066 - val_loss: 0.0056\n",
      "Epoch 22/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 23/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 24/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0024 - val_loss: 0.0053\n",
      "Epoch 25/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 26/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 27/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 28/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 29/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 30/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 31/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0044 - val_loss: 0.0055\n",
      "Epoch 32/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 33/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 34/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0022 - val_loss: 0.0053\n",
      "Epoch 35/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 36/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0061 - val_loss: 0.0054\n",
      "Epoch 37/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 38/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 39/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 40/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 41/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 42/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 43/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 44/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 45/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 46/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0053\n",
      "Epoch 47/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 48/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 49/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 50/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 51/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 52/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 53/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 54/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 55/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 56/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 57/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0024 - val_loss: 0.0053\n",
      "Epoch 58/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 59/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 60/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 61/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 62/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 63/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 64/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 65/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 66/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 67/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0022 - val_loss: 0.0053\n",
      "Epoch 68/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 69/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 70/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 71/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0023 - val_loss: 0.0053\n",
      "Epoch 72/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0029 - val_loss: 0.0053\n",
      "Epoch 73/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 74/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 75/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 76/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 77/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 78/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 80/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0024 - val_loss: 0.0053\n",
      "Epoch 81/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 82/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 83/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 84/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 85/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0023 - val_loss: 0.0053\n",
      "Epoch 86/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 87/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 88/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0022 - val_loss: 0.0053\n",
      "Epoch 89/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 90/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 91/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0024 - val_loss: 0.0053\n",
      "Epoch 92/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 93/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 94/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 95/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 96/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 97/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 98/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 99/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 100/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0053\n",
      "Epoch 101/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 102/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 103/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 104/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 105/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 106/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 107/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 108/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 109/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 110/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 111/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 112/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 113/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 114/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0018 - val_loss: 0.0053\n",
      "Epoch 115/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 116/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 117/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 118/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 119/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0061 - val_loss: 0.0054\n",
      "Epoch 120/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 121/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 122/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 123/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 124/1000\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 125/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 126/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 127/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 128/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 129/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 130/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 131/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 132/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 133/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 134/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 135/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 136/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 137/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 138/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 139/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 140/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 141/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 142/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 143/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 144/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 145/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 146/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 147/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 148/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 149/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0031 - val_loss: 0.0053\n",
      "Epoch 150/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 151/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 152/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 153/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 154/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 155/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0028 - val_loss: 0.0053\n",
      "Epoch 156/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 157/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 158/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0022 - val_loss: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 160/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 161/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 162/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 163/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 164/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 165/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 166/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 167/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 168/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 169/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 170/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0050 - val_loss: 0.0053\n",
      "Epoch 171/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 172/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 173/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 174/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 175/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0075 - val_loss: 0.0053\n",
      "Epoch 176/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 177/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 178/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 179/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 180/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 181/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 182/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0061 - val_loss: 0.0054\n",
      "Epoch 183/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 184/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 185/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0066 - val_loss: 0.0054\n",
      "Epoch 186/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 187/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 188/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 189/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 190/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 191/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 192/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 193/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 194/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 195/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0035 - val_loss: 0.0053\n",
      "Epoch 196/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 197/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0053\n",
      "Epoch 198/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 199/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 200/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0032 - val_loss: 0.0053\n",
      "Epoch 201/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 202/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 203/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0064 - val_loss: 0.0054\n",
      "Epoch 204/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 205/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 206/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 207/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 208/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 209/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 210/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 211/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 212/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 213/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 214/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 215/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 216/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 217/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 218/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 219/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 220/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 221/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 222/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 223/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 224/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 225/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 226/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 227/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 228/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 229/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 230/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 231/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 232/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 233/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 234/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 235/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 236/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 237/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 238/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 239/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 240/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 241/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 242/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 243/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 244/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 245/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 246/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 247/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 248/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 249/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 250/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 251/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 252/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 253/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 254/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 255/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 256/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 257/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 258/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 259/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 260/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 261/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0012 - val_loss: 0.0054\n",
      "Epoch 262/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 263/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 264/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 265/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 266/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 267/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 268/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 269/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 270/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 271/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 272/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 273/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 274/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 275/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 276/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 277/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 278/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 279/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0077 - val_loss: 0.0054\n",
      "Epoch 280/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 281/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 282/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 283/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 284/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 285/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 286/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 287/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 288/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 289/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 290/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 291/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 292/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 293/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 294/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 295/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 296/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 297/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 298/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 299/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 300/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 301/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 302/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 303/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 304/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 305/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 306/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 307/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 308/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 309/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 310/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 311/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 312/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 313/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 314/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 315/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 316/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 317/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 318/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 319/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 320/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 321/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 322/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 323/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 324/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 325/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 326/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 327/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 328/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 329/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 330/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 331/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 332/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 333/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 334/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 335/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 336/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 337/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 338/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 339/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 340/1000\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 341/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 342/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 343/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 344/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 345/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 346/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 347/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 348/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 349/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 350/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 351/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0068 - val_loss: 0.0054\n",
      "Epoch 352/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 353/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 354/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 355/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 356/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 357/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 358/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 359/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 360/1000\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 361/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 362/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 363/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 364/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 365/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 366/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 367/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 368/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 369/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 370/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 371/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 372/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 373/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 374/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 375/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 376/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 377/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 378/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 379/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 380/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 381/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 382/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 383/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 384/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 385/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 386/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 387/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 388/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 389/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 390/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 391/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 392/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 393/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 394/1000\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 395/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 396/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 397/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 398/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 399/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 400/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 401/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0074 - val_loss: 0.0054\n",
      "Epoch 402/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 403/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 404/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 405/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 406/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 407/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 408/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 409/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 410/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 411/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 412/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 413/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 414/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 415/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0078 - val_loss: 0.0054\n",
      "Epoch 416/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 417/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 418/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 419/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0062 - val_loss: 0.0054\n",
      "Epoch 420/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 421/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 422/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 423/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 424/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 425/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 426/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 427/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 428/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 429/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 430/1000\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 431/1000\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 432/1000\n",
      "32/32 [==============================] - 3s 97ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 433/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 434/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 435/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 436/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 437/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 438/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 439/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 440/1000\n",
      "32/32 [==============================] - 3s 96ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 441/1000\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 442/1000\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 443/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 444/1000\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 445/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 446/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 447/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 448/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 449/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 450/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 451/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 452/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0037 - val_loss: 0.0054oss\n",
      "Epoch 453/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 454/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 455/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 456/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 457/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 458/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 459/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 460/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 461/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 462/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 463/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 464/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 465/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 466/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 467/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 468/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 469/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 470/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 471/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 472/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 473/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 474/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 475/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 476/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 477/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 478/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 479/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 480/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 481/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 482/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 483/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 484/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 485/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 486/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 487/1000\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 488/1000\n",
      "32/32 [==============================] - 3s 101ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 489/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 490/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 491/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 492/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 493/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 494/1000\n",
      "32/32 [==============================] - 3s 101ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 495/1000\n",
      "32/32 [==============================] - 3s 100ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 496/1000\n",
      "32/32 [==============================] - 3s 105ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 497/1000\n",
      "32/32 [==============================] - 3s 100ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 498/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 499/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 500/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 501/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 502/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 503/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 504/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 505/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 506/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 507/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 508/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 509/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 510/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 511/1000\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 512/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0071 - val_loss: 0.0054\n",
      "Epoch 513/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 514/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 515/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 516/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 517/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 518/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 519/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 520/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 521/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 522/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 523/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 524/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0065 - val_loss: 0.0054\n",
      "Epoch 525/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 526/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 527/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 528/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 529/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 530/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 531/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 532/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 533/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 534/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 535/1000\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 536/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 537/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 538/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 539/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 540/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 541/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0012 - val_loss: 0.0054\n",
      "Epoch 542/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 543/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 544/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 545/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 546/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 547/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 548/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 549/1000\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 550/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 551/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 552/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 553/1000\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 554/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 555/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 556/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 557/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 558/1000\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 559/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 560/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 561/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 562/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 563/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0074 - val_loss: 0.0054\n",
      "Epoch 564/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 565/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 566/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 567/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 568/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 569/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 570/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 571/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 572/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 573/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 574/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 575/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 576/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 577/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 578/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 579/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 580/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 581/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 582/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 583/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 584/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 585/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 586/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 587/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 588/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 589/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 590/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 591/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 592/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 593/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 594/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 595/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 596/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 597/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 598/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 599/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0075 - val_loss: 0.0054\n",
      "Epoch 600/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 601/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 602/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 603/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 604/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 605/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 606/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 607/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 608/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 609/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 610/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0012 - val_loss: 0.0054\n",
      "Epoch 611/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 612/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 613/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 614/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 615/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 616/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 617/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 618/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 619/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 620/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 621/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 622/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 623/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 624/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 625/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 626/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 627/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 628/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 629/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 630/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 631/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 632/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 633/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 634/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 635/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 636/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 637/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 638/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 639/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 640/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 641/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 642/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 643/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 644/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 645/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 646/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 647/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 648/1000\n",
      "32/32 [==============================] - 3s 97ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 649/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 650/1000\n",
      "32/32 [==============================] - 3s 88ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 651/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 652/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 653/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 654/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 655/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 656/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 657/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 658/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 659/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 660/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 661/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 662/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 663/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 664/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 665/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 666/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 667/1000\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 668/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0011 - val_loss: 0.0054\n",
      "Epoch 669/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 670/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 671/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 672/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 673/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 674/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 675/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 676/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 677/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 678/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 679/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 680/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 681/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 682/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 683/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 684/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 685/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 686/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 687/1000\n",
      "32/32 [==============================] - 3s 87ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 688/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 689/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 690/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 691/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 692/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 693/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 694/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 695/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 696/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 697/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 698/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 699/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 700/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 701/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 702/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 703/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 704/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 705/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 706/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 707/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 708/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 709/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 710/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 711/1000\n",
      "32/32 [==============================] - 3s 91ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 712/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 713/1000\n",
      "32/32 [==============================] - 3s 98ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 714/1000\n",
      "32/32 [==============================] - 3s 98ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 715/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 716/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 717/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 718/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 719/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 720/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 721/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 722/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 723/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 724/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 725/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 726/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 727/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 728/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 729/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 730/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 731/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 732/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 733/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 734/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 735/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 736/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 737/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 738/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 739/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 740/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 741/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 742/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 743/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0012 - val_loss: 0.0054\n",
      "Epoch 744/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 745/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 746/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 747/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 748/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 749/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 750/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 751/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 752/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 753/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 754/1000\n",
      "32/32 [==============================] - 3s 93ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 755/1000\n",
      "32/32 [==============================] - 3s 91ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 756/1000\n",
      "32/32 [==============================] - 3s 92ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 757/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 758/1000\n",
      "32/32 [==============================] - 3s 94ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 759/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 760/1000\n",
      "32/32 [==============================] - 3s 89ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 761/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 762/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 763/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 764/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 765/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 766/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 767/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 768/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 769/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 770/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 771/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 772/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 773/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 774/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 775/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 776/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 777/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 778/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 779/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 780/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 781/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 782/1000\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 783/1000\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 784/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 785/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 786/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 787/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 788/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 789/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 790/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 791/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 792/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0067 - val_loss: 0.0054\n",
      "Epoch 793/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 794/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0012 - val_loss: 0.0054\n",
      "Epoch 795/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 796/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 797/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 798/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 799/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 800/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 801/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 802/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 803/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 804/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 805/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 806/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 807/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0060 - val_loss: 0.0054\n",
      "Epoch 808/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 809/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 810/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 811/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 812/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 813/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 814/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 815/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 816/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 817/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 818/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 819/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 820/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 821/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 822/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 823/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 824/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 825/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 826/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 827/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0061 - val_loss: 0.0054\n",
      "Epoch 828/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 829/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 830/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 831/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 832/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 833/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 834/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 835/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 836/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 837/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 838/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 839/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 840/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 841/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 842/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 843/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 844/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 845/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 846/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 847/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 848/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 849/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 850/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 851/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 852/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 853/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 854/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 855/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 856/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 857/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 858/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 859/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 860/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 861/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 862/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 863/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0064 - val_loss: 0.0054\n",
      "Epoch 864/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 865/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 866/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 867/1000\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 868/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 869/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 870/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 871/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 872/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0075 - val_loss: 0.0054\n",
      "Epoch 873/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 874/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 875/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 876/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 877/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 878/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 879/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 880/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 881/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 882/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 883/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 884/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 885/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 886/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 887/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 888/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 889/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 890/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 891/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 892/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 893/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 894/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 895/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 896/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 897/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 898/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 899/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 900/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 901/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 902/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 903/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 904/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 905/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 906/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 907/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 908/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 909/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 910/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 911/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 912/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 913/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 914/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 915/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 916/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 917/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 918/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 919/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 920/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 921/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 922/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 923/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 924/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 925/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 926/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 927/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 928/1000\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 929/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 930/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 931/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 932/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 933/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 934/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 935/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 936/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 937/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 938/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 939/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 940/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 941/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 942/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 943/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 944/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0033 - val_loss: 0.0054\n",
      "Epoch 945/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 946/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 947/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 948/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 949/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 950/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 951/1000\n",
      "32/32 [==============================] - 2s 75ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 952/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0062 - val_loss: 0.0054\n",
      "Epoch 953/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0048 - val_loss: 0.0054\n",
      "Epoch 954/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 955/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0024 - val_loss: 0.0054\n",
      "Epoch 956/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 957/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 958/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 959/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 960/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 961/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 962/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 963/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 964/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 965/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 966/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0044 - val_loss: 0.0054\n",
      "Epoch 967/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0031 - val_loss: 0.0054\n",
      "Epoch 968/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 969/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 970/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 971/1000\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 972/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 973/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0012 - val_loss: 0.0054\n",
      "Epoch 974/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 975/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 976/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 977/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 978/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 979/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 980/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0054 - val_loss: 0.0054\n",
      "Epoch 981/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0029 - val_loss: 0.0054\n",
      "Epoch 982/1000\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 983/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0089 - val_loss: 0.0054\n",
      "Epoch 984/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 985/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 986/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 987/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 988/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 989/1000\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0056 - val_loss: 0.0054\n",
      "Epoch 990/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 991/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 992/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0017 - val_loss: 0.0054\n",
      "Epoch 993/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 994/1000\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0038 - val_loss: 0.0054\n",
      "Epoch 995/1000\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0028 - val_loss: 0.0054\n",
      "Epoch 996/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 997/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0067 - val_loss: 0.0054\n",
      "Epoch 998/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 999/1000\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 1000/1000\n",
      "32/32 [==============================] - 3s 78ms/step - loss: 0.0058 - val_loss: 0.0054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See Plot for predicted vs. actuals\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Vs. Predicted\n",
      "Validation    36515.000000\n",
      "Predicted     54208.806627\n",
      "dtype: float64\n",
      "See Plot for Future Predictions\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>announcements</th>\n",
       "      <th>historical_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>902.763538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>903.502017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>903.479534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>903.481263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>903.484722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>903.486452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>903.486452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>903.484722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>903.488181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>903.489911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>903.491640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>903.491640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>903.491640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>903.491640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>903.489911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    announcements  historical_flag\n",
       "59     902.763538                0\n",
       "60     903.502017                0\n",
       "61     903.479534                0\n",
       "62     903.481263                0\n",
       "63     903.484722                0\n",
       "64     903.486452                0\n",
       "65     903.486452                0\n",
       "66     903.484722                0\n",
       "67     903.488181                0\n",
       "68     903.489911                0\n",
       "69     903.491640                0\n",
       "70     903.491640                0\n",
       "71     903.491640                0\n",
       "72     903.491640                0\n",
       "73     903.489911                0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "\n",
    "adr = get('https://api.ipify.org').text\n",
    "sourceip = \"https://stat.ripe.net/data/whois/data.json?resource=\"+adr+\"%2F24\"\n",
    "responseip = requests.get(sourceip).json()\n",
    "#asn_code = responseip[\"data\"][\"irr_records\"][0][1][\"value\"]\n",
    "asn_code =206472\n",
    "url = 'https://stat.ripe.net/data/bgp-update-activity/data.json?endtime=2022-04-11T12%3A00%3A00&hide_empty_samples=false&max_samples=5000&resource=AS'+str(asn_code)+'&starttime=2021-04-11T00%3A00%3A00'\n",
    "r = requests.get(url)\n",
    "json = r.json()\n",
    "dat = pd.DataFrame(json['data']['updates'])\n",
    "\n",
    "\n",
    "split = 0.8\n",
    "sequence_length = 60\n",
    "\n",
    "data_prep =Data_Prep(dataset = dat)\n",
    "rnn_df, validation_df = data_prep.preprocess_rnn(date_colname = 'starttime', numeric_colname = 'announcements', pred_set_timesteps = 60)\n",
    "\n",
    "\n",
    "series_prep = Series_Prep(rnn_df =  rnn_df, numeric_colname = 'announcements')\n",
    "window, X_min, X_max = series_prep.make_window(sequence_length = sequence_length, \n",
    "                                               train_test_split = split, \n",
    "                                               return_original_x = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = series_prep.reshape_window(window, train_test_split = split)\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#                 Building the LSTM\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import ReduceLROnPlateau #Learning rate scheduler for when we reach plateaus\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100)\n",
    "\n",
    "# Reset model if we want to re-train with different splits\n",
    "def reset_weights(model):\n",
    "    import keras.backend as K\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'): \n",
    "            layer.kernel.initializer.run(session=session)\n",
    "        if hasattr(layer, 'bias_initializer'):\n",
    "            layer.bias.initializer.run(session=session)  \n",
    "\n",
    "\n",
    "# Epochs and validation split\n",
    "EPOCHS = 1000\n",
    "validation = 0.05\n",
    "\n",
    "# Instantiate the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer.... the input shape is (Sample, seq_len-1, 1)\n",
    "model.add(LSTM(\n",
    "        input_shape = (sequence_length-1, 1), return_sequences = True,\n",
    "        units = 100))\n",
    "\n",
    "# Add the second layer.... the input shape is (Sample, seq_len-1, 1)\n",
    "model.add(LSTM(\n",
    "        input_shape = (sequence_length-1, 1), \n",
    "        units = 100))\n",
    "\n",
    "# Add the output layer, simply one unit\n",
    "model.add(Dense(\n",
    "        units = 1,\n",
    "        activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "\n",
    "# History object for plotting our model loss by epoch\n",
    "history = model.fit(X_train, y_train, epochs = EPOCHS, validation_split = validation,\n",
    "          callbacks = [rlrop])\n",
    "# Loss History\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#              Predicting the future\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "# Creating our future object\n",
    "future = Predict_Future(X_test  = X_test, validation_df = validation_df, lstm_model = model)\n",
    "# Checking its accuracy on our training set\n",
    "future.predicted_vs_actual(X_min = X_min, X_max = X_max, numeric_colname = 'announcements')\n",
    "# Predicting 'x' timesteps out\n",
    "future.predict_future(X_min = X_min, X_max = X_max, numeric_colname = 'announcements', timesteps_to_predict = 15, return_future = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting time_series\n",
      "  Downloading time_series-0.2-py3-none-any.whl (3.5 kB)\n",
      "Installing collected packages: time-series\n",
      "Successfully installed time-series-0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\viviane\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Data_Prep:\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    \n",
    "    def preprocess_rnn(self, date_colname, numeric_colname, pred_set_timesteps):\n",
    "        features = (create_series(self.dataset, numeric_colname, date_colname)).sort_index()\n",
    "        rnn_df = features.groupby(features.index).sum()\n",
    "        \n",
    "        # Filter out 'n' timesteps for prediction purposes\n",
    "        timestep_idx = len(rnn_df)-pred_set_timesteps\n",
    "        validation_df = rnn_df.iloc[timestep_idx:]\n",
    "        rnn_df = rnn_df.iloc[1:timestep_idx,]\n",
    "        \n",
    "        # Dickey Fuller Test\n",
    "        print(\"Summary Statistics - ADF Test For Stationarity\\n\")\n",
    "        if stationarity_test(X = rnn_df[numeric_colname], return_p=True, print_res = False) > 0.05:\n",
    "            print(\"P Value is high. Consider Differencing: \" + str(stationarity_test(X = rnn_df[numeric_colname], return_p = True, print_res = False)))\n",
    "        else:\n",
    "            stationarity_test(X = rnn_df[numeric_colname])\n",
    "        \n",
    "        # Sorting\n",
    "        rnn_df = rnn_df.sort_index(ascending = True)\n",
    "        rnn_df = rnn_df.reset_index()\n",
    "        \n",
    "        return rnn_df, validation_df\n",
    "    \n",
    "    \n",
    "class Series_Prep:\n",
    "    \n",
    "    def __init__(self, rnn_df, numeric_colname):\n",
    "        self.rnn_df = rnn_df\n",
    "        self.numeric_colname = numeric_colname\n",
    "\n",
    "    def make_window(self, sequence_length, train_test_split, return_original_x = True):\n",
    "        \n",
    "        # Create the initial results df with a look_back of 60 days\n",
    "        result = []\n",
    "        \n",
    "        # 3D Array\n",
    "        for index in range(len(self.rnn_df) - sequence_length):\n",
    "            result.append(self.rnn_df[self.numeric_colname][index: index + sequence_length])  \n",
    "        \n",
    "        # Getting the initial train_test split for our min/max val scalar\n",
    "        train_test_split = 0.9\n",
    "        row = int(round(train_test_split * np.array(result).shape[0]))\n",
    "        train = np.array(result)[:row, :]\n",
    "        X_train = train[:, :-1]\n",
    "        \n",
    "        # Manual MinMax Scaler\n",
    "        X_min = X_train.min()\n",
    "        X_max = X_train.max()\n",
    "        \n",
    "        # keep the originals in case\n",
    "        X_min_orig = X_train.min()\n",
    "        X_max_orig = X_train.max()\n",
    "        \n",
    "        # Minmax scaler and a reverse method\n",
    "        def minmax(X):\n",
    "            return (X-X_min) / (X_max - X_min)\n",
    "        \n",
    "        def reverse_minmax(X):\n",
    "            return X * (X_max-X_min) + X_min\n",
    "        \n",
    "        # Method for Scaler for each window in our 3D array\n",
    "        def minmax_windows(window_data):\n",
    "            normalised_data = []\n",
    "            for window in window_data:\n",
    "                window.index = range(sequence_length)\n",
    "                normalised_window = [((minmax(p))) for p in window]\n",
    "                normalised_data.append(normalised_window)\n",
    "            return normalised_data\n",
    "        \n",
    "        # minmax the windows\n",
    "        result = minmax_windows(result)\n",
    "        # Convert to 2D array\n",
    "        result = np.array(result)\n",
    "        if return_original_x:\n",
    "            return result, X_min_orig, X_max_orig\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def reshape_window(window, train_test_split = 0.8):\n",
    "        # Train/test for real this time\n",
    "        row = round(train_test_split * window.shape[0])\n",
    "        train = window[:row, :]\n",
    "        \n",
    "        # Get the sets\n",
    "        X_train = train[:, :-1]\n",
    "        y_train = train[:, -1]\n",
    "        X_test = window[row:, :-1]\n",
    "        y_test = window[row:, -1]\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "        y_train = np.reshape(y_train, (-1,1))\n",
    "        y_test = np.reshape(y_test, (-1,1))\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "class Predict_Future:\n",
    "\n",
    "\n",
    "    def __init__(self, X_test, validation_df, lstm_model):\n",
    "        self.X_test = X_test\n",
    "        self.validation_df = validation_df\n",
    "        self.lstm_model = lstm_model\n",
    "        \n",
    "    def predicted_vs_actual(self, X_min, X_max, numeric_colname):\n",
    "        \n",
    "        curr_frame = self.X_test[len(self.X_test)-1]\n",
    "        future = []\n",
    "        \n",
    "        for i in range(len(self.validation_df)):\n",
    "              # append the prediction to our empty future list\n",
    "             future.append(self.lstm_model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "              # insert our predicted point to our current frame\n",
    "             curr_frame = np.insert(curr_frame, len(self.X_test[0]), future[-1], axis=0)\n",
    "              # push the frame up one to make it progress into the future\n",
    "             curr_frame = curr_frame[1:]\n",
    "        \n",
    "        def reverse_minmax(X, X_max = X_max, X_min = X_min):\n",
    "            return X * (X_max-X_min) + X_min\n",
    "\n",
    "        # Plot \n",
    "        reverse_curr_frame = pd.DataFrame({numeric_colname: [reverse_minmax(x) for x in self.X_test[len(self.X_test)-1]],\n",
    "                                           \"historical_flag\":1})\n",
    "        reverse_future = pd.DataFrame({numeric_colname: [reverse_minmax(x) for x in future],\n",
    "                                           \"historical_flag\":0})\n",
    "        \n",
    "        # Change the indicies! Only for FUTURE predictions\n",
    "        # reverse_future.index += len(reverse_curr_frame)\n",
    "        \n",
    "        print(\"See Plot for predicted vs. actuals\")\n",
    "        plt.plot(reverse_curr_frame[numeric_colname])\n",
    "        plt.plot(reverse_future[numeric_colname])\n",
    "        plt.title(\"Predicted Points Vs. Actuals (Validation)\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Check accuracy vs. actuals\n",
    "        comparison_df = pd.DataFrame({\"Validation\": self.validation_df[numeric_colname],\n",
    "                                      \"Predicted\": [reverse_minmax(x) for x in future]})\n",
    "        print(\"Validation Vs. Predicted\")\n",
    "        print(comparison_df.sum())\n",
    "        \n",
    "        \n",
    "    def predict_future(self, X_min, X_max, numeric_colname, timesteps_to_predict, return_future = True):\n",
    "    \n",
    "        curr_frame = self.X_test[len(self.X_test)-1]\n",
    "        future = []\n",
    "        \n",
    "        for i in range(timesteps_to_predict):\n",
    "              # append the prediction to our empty future list\n",
    "             future.append(self.lstm_model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "              # insert our predicted point to our current frame\n",
    "             curr_frame = np.insert(curr_frame, len(self.X_test[0]), future[-1], axis=0)\n",
    "              # push the frame up one to make it progress into the future\n",
    "             curr_frame = curr_frame[1:]\n",
    "        \n",
    "        def reverse_minmax(X, X_max = X_max, X_min = X_min):\n",
    "            return X * (X_max-X_min) + X_min\n",
    "\n",
    "        # Reverse the original frame and the future frame\n",
    "        reverse_curr_frame = pd.DataFrame({numeric_colname: [reverse_minmax(x) for x in self.X_test[len(self.X_test)-1]],\n",
    "                                           \"historical_flag\":1})\n",
    "        reverse_future = pd.DataFrame({numeric_colname: [reverse_minmax(x) for x in future],\n",
    "                                           \"historical_flag\":0})\n",
    "        \n",
    "        # Change the indicies to show prediction next to the actuals in orange\n",
    "        reverse_future.index += len(reverse_curr_frame)\n",
    "        \n",
    "        print(\"See Plot for Future Predictions\")\n",
    "        plt.plot(reverse_curr_frame[numeric_colname])\n",
    "        plt.plot(reverse_future[numeric_colname])\n",
    "        plt.title(\"Predicted Future of \"+ str(timesteps_to_predict) + \" days\")\n",
    "        plt.show()\n",
    "        \n",
    "        if return_future:\n",
    "            return reverse_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_series(df, xcol, datecol):\n",
    "    # Create a dataframe with the features and the date time as the index\n",
    "    features_considered = [xcol]\n",
    "    features = df[features_considered]\n",
    "    features.index = df[datecol]\n",
    "    features.head()\n",
    "    features.plot(subplots=True)\n",
    "    return features\n",
    "\n",
    "\n",
    "# X is the series to test\n",
    "# log_x asks whether to log X prior to testing or not\n",
    "def stationarity_test(X, log_x = \"Y\", return_p = False, print_res = True):\n",
    "    \n",
    "    # If X isn't logged, we need to log it for better results\n",
    "    if log_x == \"Y\":\n",
    "        X = np.log(X[X>0])\n",
    "    \n",
    "    # Once we have the series as needed we can do the ADF test\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    dickey_fuller = adfuller(X)\n",
    "    \n",
    "    if print_res:\n",
    "    # If ADF statistic is < our 1% critical value (sig level) we can conclude it's not a fluke (ie low P val / reject H(0))\n",
    "        print('ADF Stat is: {}.'.format(dickey_fuller[0]))\n",
    "        # A lower p val means we can reject the H(0) that our data is NOT stationary\n",
    "        print('P Val is: {}.'.format(dickey_fuller[1]))\n",
    "        print('Critical Values (Significance Levels): ')\n",
    "        for key,val in dickey_fuller[4].items():\n",
    "            print(key,\":\",round(val,3))\n",
    "            \n",
    "    if return_p:\n",
    "        return dickey_fuller[1]\n",
    "    \n",
    "# Differencing the data    \n",
    "def difference(X):\n",
    "    diff = X.diff()\n",
    "    plt.plot(diff)\n",
    "    plt.show()\n",
    "    return diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
